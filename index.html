<p>7 <a href="https://arxiv.org/abs/2403.10171">[2403.10171] AUTONODE: A Neuro-Graphic Self-Learnable Engine for Cognitive GUI Automation</a></p>
<p>AUTONODE:面向认知GUI自动化的神经图形自学习引擎</p>
<details>
<summary>查看更多</summary>
<pre><code>
Date: Fri, 15 Mar 2024 10:27:17 GMT
Authors: Arkajit Datta, Tushar Verma, Rajat Chawla
Abstract
In recent advancements within the domain of Large Language Models (LLMs), there has been a notable emergence of agents capable of addressing Robotic Process Automation (RPA) challenges through enhanced cognitive capabilities and sophisticated reasoning. This development heralds a new era of scalability and human-like adaptability in goal attainment. In this context, we introduce AUTONODE (Autonomous User-interface Transformation through Online Neuro-graphic Operations and Deep Exploration). AUTONODE employs advanced neuro-graphical techniques to facilitate autonomous navigation and task execution on web interfaces, thereby obviating the necessity for predefined scripts or manual intervention. Our engine empowers agents to comprehend and implement complex workflows, adapting to dynamic web environments with unparalleled efficiency.
Our methodology synergizes cognitive functionalities with robotic automation, endowing AUTONODE with the ability to learn from experience. We have integrated an exploratory module, DoRA (Discovery and mapping Operation for graph Retrieval Agent), which is instrumental in constructing a knowledge graph that the engine utilizes to optimize its actions and achieve objectives with minimal supervision. The versatility and efficacy of AUTONODE are demonstrated through a series of experiments, highlighting its proficiency in managing a diverse array of web-based tasks, ranging from data extraction to transaction processing.
</code></pre>
</details>
<hr />
<p>7 <a href="https://arxiv.org/abs/2403.10171">[2403.10171] AUTONODE: A Neuro-Graphic Self-Learnable Engine for Cognitive GUI Automation</a></p>
<p>AUTONODE:面向认知GUI自动化的神经图形自学习引擎</p>
<details>
<summary>查看更多</summary>
<pre><code>
Date: Fri, 15 Mar 2024 10:27:17 GMT
Authors: Arkajit Datta, Tushar Verma, Rajat Chawla
Abstract
In recent advancements within the domain of Large Language Models (LLMs), there has been a notable emergence of agents capable of addressing Robotic Process Automation (RPA) challenges through enhanced cognitive capabilities and sophisticated reasoning. This development heralds a new era of scalability and human-like adaptability in goal attainment. In this context, we introduce AUTONODE (Autonomous User-interface Transformation through Online Neuro-graphic Operations and Deep Exploration). AUTONODE employs advanced neuro-graphical techniques to facilitate autonomous navigation and task execution on web interfaces, thereby obviating the necessity for predefined scripts or manual intervention. Our engine empowers agents to comprehend and implement complex workflows, adapting to dynamic web environments with unparalleled efficiency.
Our methodology synergizes cognitive functionalities with robotic automation, endowing AUTONODE with the ability to learn from experience. We have integrated an exploratory module, DoRA (Discovery and mapping Operation for graph Retrieval Agent), which is instrumental in constructing a knowledge graph that the engine utilizes to optimize its actions and achieve objectives with minimal supervision. The versatility and efficacy of AUTONODE are demonstrated through a series of experiments, highlighting its proficiency in managing a diverse array of web-based tasks, ranging from data extraction to transaction processing.
</code></pre>
</details>
<hr />
<p>14 <a href="https://arxiv.org/abs/2403.09674">[2403.09674] Navigating the Peril of Generated Alternative Facts: A ChatGPT-4 Fabricated Omega Variant Case as a Cautionary Tale in Medical Misinformation</a></p>
<p>探索产生的其他事实的危险:一个ChatGPT-4编造的Omega变体案例，作为医学错误信息中的警示故事</p>
<details>
<summary>查看更多</summary>
<pre><code>
Date: Sun, 4 Feb 2024 13:21:19 GMT
Authors: Malik Sallam, Jan Egger, Rainer Roehrig, Behrus Puladi
Abstract
In an era where artificial intelligence (AI) intertwines with medical research, the delineation of truth becomes increasingly complex. This study ostensibly examines a purported novel SARS-CoV-2 variant, dubbed the Omega variant, showcasing 31 unique mutations in the S gene region. However, the real undercurrent of this narrative is a demonstration of the ease with which AI, specifically ChatGPT-4, can fabricate convincing yet entirely fictional scientific data. The so-called Omega variant was identified in a fully vaccinated, previously infected 35-year-old male presenting with severe COVID-19 symptoms. Through a detailed, albeit artificial, genomic analysis and contact tracing, this study mirrors the rigorous methodology of genuine case reports, thereby setting the stage for a compelling but entirely constructed narrative. The entire case study was generated by ChatGPT-4, a large language model by OpenAI. The fabricated Omega variant features an ensemble of mutations, including N501Y and E484K, known for enhancing ACE2 receptor affinity, alongside L452R and P681H, ostensibly indicative of immune evasion.
This variant's contrived interaction dynamics - severe symptoms in a vaccinated individual versus mild ones in unvaccinated contacts - were designed to mimic real-world complexities, including suggestions of antibody-dependent enhancement (ADE). While the Omega variant is a product of AI-generated fiction, the implications of this exercise are real and profound. The ease with which AI can generate believable but false scientific information, as illustrated in this case, raises significant concerns about the potential for misinformation in medicine. This study, therefore, serves as a cautionary tale, emphasizing the necessity for critical evaluation of sources, especially in an age where AI tools like ChatGPT are becoming increasingly sophisticated and widespread in their use.
</code></pre>
</details>
<hr />
<p>15 <a href="https://arxiv.org/abs/2403.09676">[2403.09676] Unmasking the Shadows of AI: Investigating Deceptive Capabilities in Large Language Models</a></p>
<p>揭开人工智能的阴影:研究大型语言模型的欺骗能力</p>
<details>
<summary>查看更多</summary>
<pre><code>
Date: Wed, 7 Feb 2024 00:21:46 GMT
Authors: Linge Guo
Abstract
This research critically navigates the intricate landscape of AI deception, concentrating on deceptive behaviours of Large Language Models (LLMs). My objective is to elucidate this issue, examine the discourse surrounding it, and subsequently delve into its categorization and ramifications. The essay initiates with an evaluation of the AI Safety Summit 2023 (ASS) and introduction of LLMs, emphasising multidimensional biases that underlie their deceptive behaviours.The literature review covers four types of deception categorised: Strategic deception, Imitation, Sycophancy, and Unfaithful Reasoning, along with the social implications and risks they entail. Lastly, I take an evaluative stance on various aspects related to navigating the persistent challenges of the deceptive AI. This encompasses considerations of international collaborative governance, the reconfigured engagement of individuals with AI, proposal of practical adjustments, and specific elements of digital education.
</code></pre>
</details>
<hr />
<p>16 <a href="https://arxiv.org/abs/2403.09702">[2403.09702] Generator-Guided Crowd Reaction Assessment</a></p>
<p>发电机引导的人群反应评估</p>
<details>
<summary>查看更多</summary>
<pre><code>
Date: Fri, 8 Mar 2024 13:05:44 GMT
Authors: Sohom Ghosh, Chung-Chi Chen, Sudip Kumar Naskar
Abstract
In the realm of social media, understanding and predicting post reach is a significant challenge. This paper presents a Crowd Reaction AssessMent (CReAM) task designed to estimate if a given social media post will receive more reaction than another, a particularly essential task for digital marketers and content writers. We introduce the Crowd Reaction Estimation Dataset (CRED), consisting of pairs of tweets from The White House with comparative measures of retweet count. The proposed Generator-Guided Estimation Approach (GGEA) leverages generative Large Language Models (LLMs), such as ChatGPT, FLAN-UL2, and Claude, to guide classification models for making better predictions. Our results reveal that a fine-tuned FLANG-RoBERTa model, utilizing a cross-encoder architecture with tweet content and responses generated by Claude, performs optimally. We further use a T5-based paraphraser to generate paraphrases of a given post and demonstrate GGEA's ability to predict which post will elicit the most reactions. We believe this novel application of LLMs provides a significant advancement in predicting social media post reach.
</code></pre>
</details>
<hr />
<p>18 <a href="https://arxiv.org/abs/2403.09704">[2403.09704] Alignment Studio: Aligning Large Language Models to Particular Contextual Regulations</a></p>
<p>Alignment Studio:将大型语言模型与特定的上下文规则对齐</p>
<details>
<summary>查看更多</summary>
<pre><code>
Date: Fri, 8 Mar 2024 21:26:49 GMT
Authors: Swapnaja Achintalwar, Ioana Baldini, Djallel Bouneffouf, Joan Byamugisha, Maria Chang, Pierre Dognin, Eitan Farchi, Ndivhuwo Makondo, Aleksandra Mojsilovic, Manish Nagireddy, Karthikeyan Natesan Ramamurthy, Inkit Padhi, Orna Raz, Jesus Rios, Prasanna Sattigeri, Moninder Singh, Siphiwe Thwala, Rosario A. Uceda-Sosa, Kush R. Varshney
Abstract
The alignment of large language models is usually done by model providers to add or control behaviors that are common or universally understood across use cases and contexts. In contrast, in this article, we present an approach and architecture that empowers application developers to tune a model to their particular values, social norms, laws and other regulations, and orchestrate between potentially conflicting requirements in context. We lay out three main components of such an Alignment Studio architecture: Framers, Instructors, and Auditors that work in concert to control the behavior of a language model. We illustrate this approach with a running example of aligning a company's internal-facing enterprise chatbot to its business conduct guidelines.
</code></pre>
</details>
<hr />
<p>19 <a href="https://arxiv.org/abs/2403.09705">[2403.09705] A Novel Nuanced Conversation Evaluation Framework for Large Language Models in Mental Health</a></p>
<p>面向心理健康大型语言模型的新颖微妙对话评估框架</p>
<details>
<summary>查看更多</summary>
<pre><code>
Date: Fri, 8 Mar 2024 23:46:37 GMT
Authors: Alexander Marrapese, Basem Suleiman, Imdad Ullah, Juno Kim
Abstract
Understanding the conversation abilities of Large Language Models (LLMs) can help lead to its more cautious and appropriate deployment. This is especially important for safety-critical domains like mental health, where someone's life may depend on the exact wording of a response to an urgent question. In this paper, we propose a novel framework for evaluating the nuanced conversation abilities of LLMs. Within it, we develop a series of quantitative metrics developed from literature on using psychotherapy conversation analysis literature. While we ensure that our framework and metrics are transferable by researchers to relevant adjacent domains, we apply them to the mental health field. We use our framework to evaluate several popular frontier LLMs, including some GPT and Llama models, through a verified mental health dataset.
Our results show that GPT4 Turbo can perform significantly more similarly to verified therapists than other selected LLMs. We conduct additional analysis to examine how LLM conversation performance varies across specific mental health topics. Our results indicate that GPT4 Turbo performs well in achieving high correlation with verified therapists in particular topics such as Parenting and Relationships. We believe our contributions will help researchers develop better LLMs that, in turn, will more positively support people's lives.
</code></pre>
</details>
<hr />
<p>27 <a href="https://arxiv.org/abs/2403.09720">[2403.09720] Fine-tuning vs Prompting, Can Language Models Understand Human Values?</a></p>
<p>微调vs提示，语言模型能理解人类的价值观吗?</p>
<details>
<summary>查看更多</summary>
<pre><code>
Date: Tue, 12 Mar 2024 08:49:31 GMT
Authors: Pingwei Sun
Abstract
Accurately handling the underlying support values in sentences is crucial for understanding the speaker's tendencies, yet it poses a challenging task in natural language understanding (NLU). In this article, we explore the potential of fine-tuning and prompt tuning in this downstream task, using the Human Value Detection 2023. Additionally, we attempt to validate whether models can effectively solve the problem based on the knowledge acquired during the pre-training stage. Simultaneously, our interest lies in the capabilities of large language models (LLMs) aligned with RLHF in this task, and some preliminary attempts are presented.
</code></pre>
</details>
<hr />
<p>32 <a href="https://arxiv.org/abs/2403.09727">[2403.09727] Investigating the performance of Retrieval-Augmented Generation and fine-tuning for the development of AI-driven knowledge-based systems</a></p>
<p>研究检索增强生成的性能，并对ai驱动的基于知识系统的开发进行微调</p>
<details>
<summary>查看更多</summary>
<pre><code>
Date: Tue, 12 Mar 2024 21:06:31 GMT
Authors: Robert Lakatos, Peter Pollner, Andras Hajdu, Tamas Joo
Abstract
The development of generative large language models (G-LLM) opened up new opportunities for the development of new types of knowledge-based systems similar to ChatGPT, Bing, or Gemini. Fine-tuning (FN) and Retrieval-Augmented Generation (RAG) are the techniques that can be used to implement domain adaptation for the development of G-LLM-based knowledge systems. In our study, using ROUGE, BLEU, METEOR scores, and cosine similarity, we compare and examine the performance of RAG and FN for the GPT-J-6B, OPT-6.7B, LlaMA, LlaMA-2 language models. Based on measurements shown on different datasets, we demonstrate that RAG-based constructions are more efficient than models produced with FN. We point out that connecting RAG and FN is not trivial, because connecting FN models with RAG can cause a decrease in performance.
Furthermore, we outline a simple RAG-based architecture which, on average, outperforms the FN models by 16% in terms of the ROGUE score, 15% in the case of the BLEU score, and 53% based on the cosine similarity. This shows the significant advantage of RAG over FN in terms of hallucination, which is not offset by the fact that the average 8% better METEOR score of FN models indicates greater creativity compared to RAG.
</code></pre>
</details>
<hr />
<p>34 <a href="https://arxiv.org/abs/2403.09732">[2403.09732] PET-SQL: A Prompt-enhanced Two-stage Text-to-SQL Framework with Cross-consistency</a></p>
<p>PET-SQL:一种基于提示增强的跨一致性两阶段Text-to-SQL框架</p>
<details>
<summary>查看更多</summary>
<pre><code>
Date: Wed, 13 Mar 2024 02:32:41 GMT
Authors: Zhishuai Li, Xiang Wang, Jingjing Zhao, Sun Yang, Guoqing Du, Xiaoru Hu, Bin Zhang, Yuxiao Ye, Ziyue Li, Rui Zhao, Hangyu Mao
Abstract
Recent advancements in Text-to-SQL (Text2SQL) emphasize stimulating the large language models (LLM) on in-context learning, achieving significant results.
Nevertheless, they face challenges when dealing with verbose database information and complex user intentions. This paper presents a two-stage framework to enhance the performance of current LLM-based natural language to SQL systems. We first introduce a novel prompt representation, called reference-enhanced representation, which includes schema information and randomly sampled cell values from tables to instruct LLMs in generating SQL queries. Then, in the first stage, question-SQL pairs are retrieved as few-shot demonstrations, prompting the LLM to generate a preliminary SQL (PreSQL). After that, the mentioned entities in PreSQL are parsed to conduct schema linking, which can significantly compact the useful information. In the second stage, with the linked schema, we simplify the prompt's schema information and instruct the LLM to produce the final SQL. Finally, as the post-refinement module, we propose using cross-consistency across different LLMs rather than self-consistency within a particular LLM. Our methods achieve new SOTA results on the Spider benchmark, with an execution accuracy of 87.6%.
</code></pre>
</details>
<hr />
<p>35 <a href="https://arxiv.org/abs/2403.09733">[2403.09733] OverleafCopilot: Empowering Academic Writing in Overleaf with Large Language Models</a></p>
<p>OverleafCopilot:用大型语言模型支持背页学术写作</p>
<details>
<summary>查看更多</summary>
<pre><code>
Date: Wed, 13 Mar 2024 07:52:31 GMT
Authors: Haomin Wen, Zhenjie Wei, Yan Lin, Jiyuan Wang, Yuxuan Liang, Huaiyu Wan
Abstract
The rapid development of Large Language Models (LLMs) has facilitated a variety of applications from different domains. In this technical report, we explore the integration of LLMs and the popular academic writing tool, Overleaf, to enhance the efficiency and quality of academic writing. To achieve the above goal, there are three challenges: i) including seamless interaction between Overleaf and LLMs, ii) establishing reliable communication with the LLM provider, and iii) ensuring user privacy. To address these challenges, we present OverleafCopilot, the first-ever tool (i.e., a browser extension) that seamlessly integrates LLMs and Overleaf, enabling researchers to leverage the power of LLMs while writing papers. Specifically, we first propose an effective framework to bridge LLMs and Overleaf. Then, we developed PromptGenius, a website for researchers to easily find and share high-quality up-to-date prompts. Thirdly, we propose an agent command system to help researchers quickly build their customizable agents. OverleafCopilot (https://chromewebstore.google.com/detail/overleaf-copilot/eoadabdpninlhkkbhngoddfjianhlghb ) has been on the Chrome Extension Store, which now serves thousands of researchers. Additionally, the code of PromptGenius is released at https://github.com/wenhaomin/ChatGPT-PromptGenius. We believe our work has the potential to revolutionize academic writing practices, empowering researchers to produce higher-quality papers in less time.
</code></pre>
</details>
<hr />
<p>36 <a href="https://arxiv.org/abs/2403.09734">[2403.09734] Do Large Language Models Solve ARC Visual Analogies Like People Do?</a></p>
<p>大型语言模型是否像人们一样解决弧线视觉类比?</p>
<details>
<summary>查看更多</summary>
<pre><code>
Date: Wed, 13 Mar 2024 09:48:13 GMT
Authors: Gustaw Opie{\l}ka, Hannes Rosenbusch, Veerle Vijverberg, Claire E. Stevenson
Abstract
The Abstraction Reasoning Corpus (ARC) is a visual analogical reasoning test designed for humans and machines (Chollet, 2019). We compared human and large language model (LLM) performance on a new child-friendly set of ARC items.
Results show that both children and adults outperform most LLMs on these tasks.
Error analysis revealed a similar "fallback" solution strategy in LLMs and young children, where part of the analogy is simply copied. In addition, we found two other error types, one based on seemingly grasping key concepts (e.g., Inside-Outside) and the other based on simple combinations of analogy input matrices. On the whole, "concept" errors were more common in humans, and "matrix" errors were more common in LLMs. This study sheds new light on LLM reasoning ability and the extent to which we can use error analyses and comparisons with human development to understand how LLMs solve visual analogies.
</code></pre>
</details>
<hr />
<p>37 <a href="https://arxiv.org/abs/2403.09738">[2403.09738] Evaluating Large Language Models as Generative User Simulators for Conversational Recommendation</a></p>
<p>评价作为会话推荐的生成式用户模拟器的大型语言模型</p>
<details>
<summary>查看更多</summary>
<pre><code>
Date: Wed, 13 Mar 2024 18:16:21 GMT
Authors: Se-eun Yoon, Zhankui He, Jessica Maria Echterhoff, Julian McAuley
Abstract
Synthetic users are cost-effective proxies for real users in the evaluation of conversational recommender systems. Large language models show promise in simulating human-like behavior, raising the question of their ability to represent a diverse population of users. We introduce a new protocol to measure the degree to which language models can accurately emulate human behavior in conversational recommendation. This protocol is comprised of five tasks, each designed to evaluate a key property that a synthetic user should exhibit: choosing which items to talk about, expressing binary preferences, expressing open-ended preferences, requesting recommendations, and giving feedback.
Through evaluation of baseline simulators, we demonstrate these tasks effectively reveal deviations of language models from human behavior, and offer insights on how to reduce the deviations with model selection and prompting strategies.
</code></pre>
</details>
<hr />
<p>38 <a href="https://arxiv.org/abs/2403.09743">[2403.09743] The Human Factor in Detecting Errors of Large Language Models: A Systematic Literature Review and Future Research Directions</a></p>
<p>大型语言模型检错中的人为因素:系统文献综述与未来研究方向</p>
<details>
<summary>查看更多</summary>
<pre><code>
Date: Wed, 13 Mar 2024 21:39:39 GMT
Authors: Christian A. Schiller
Abstract
The launch of ChatGPT by OpenAI in November 2022 marked a pivotal moment for Artificial Intelligence, introducing Large Language Models (LLMs) to the mainstream and setting new records in user adoption. LLMs, particularly ChatGPT, trained on extensive internet data, demonstrate remarkable conversational capabilities across various domains, suggesting a significant impact on the workforce. However, these models are susceptible to errors - "hallucinations" and omissions, generating incorrect or incomplete information.
This poses risks especially in contexts where accuracy is crucial, such as legal compliance, medicine or fine-grained process frameworks.
There are both technical and human solutions to cope with this isse. This paper explores the human factors that enable users to detect errors in LLM outputs, a critical component in mitigating risks associated with their use in professional settings. Understanding these factors is essential for organizations aiming to leverage LLM technology efficiently, guiding targeted training and deployment strategies to enhance error detection by users. This approach not only aims to optimize the use of LLMs but also to prevent potential downstream issues stemming from reliance on inaccurate model responses. The research emphasizes the balance between technological advancement and human insight in maximizing the benefits of LLMs while minimizing the risks, particularly in areas where precision is paramount.
This paper performs a systematic literature research on this research topic, analyses and synthesizes the findings, and outlines future research directions.
Literature selection cut-off date is January 11th 2024.
</code></pre>
</details>
<hr />
<p>39 <a href="https://arxiv.org/abs/2403.09744">[2403.09744] Evaluating the Application of Large Language Models to Generate Feedback in Programming Education</a></p>
<p>编程教育中大型语言模型生成反馈的应用评估</p>
<details>
<summary>查看更多</summary>
<pre><code>
Date: Wed, 13 Mar 2024 23:14:35 GMT
Authors: Sven Jacobs and Steffen Jaschke
Abstract
This study investigates the application of large language models, specifically GPT-4, to enhance programming education. The research outlines the design of a web application that uses GPT-4 to provide feedback on programming tasks, without giving away the solution. A web application for working on programming tasks was developed for the study and evaluated with 51 students over the course of one semester. The results show that most of the feedback generated by GPT-4 effectively addressed code errors. However, challenges with incorrect suggestions and hallucinated issues indicate the need for further improvements.
</code></pre>
</details>
<hr />
<p>40 <a href="https://arxiv.org/abs/2403.09747">[2403.09747] Re-Search for The Truth: Multi-round Retrieval-augmented Large Language Models are Strong Fake News Detectors</a></p>
<p>真相的研究:多轮检索增强的大型语言模型是强大的假新闻检测器</p>
<details>
<summary>查看更多</summary>
<pre><code>
Date: Thu, 14 Mar 2024 00:35:39 GMT
Authors: Guanghua Li, Wensheng Lu, Wei Zhang, Defu Lian, Kezhong Lu, Rui Mao, Kai Shu, Hao Liao
Abstract
The proliferation of fake news has had far-reaching implications on politics, the economy, and society at large. While Fake news detection methods have been employed to mitigate this issue, they primarily depend on two essential elements: the quality and relevance of the evidence, and the effectiveness of the verdict prediction mechanism. Traditional methods, which often source information from static repositories like Wikipedia, are limited by outdated or incomplete data, particularly for emerging or rare claims. Large Language Models (LLMs), known for their remarkable reasoning and generative capabilities, introduce a new frontier for fake news detection. However, like traditional methods, LLM-based solutions also grapple with the limitations of stale and long-tail knowledge. Additionally, retrieval-enhanced LLMs frequently struggle with issues such as low-quality evidence retrieval and context length constraints. To address these challenges, we introduce a novel, retrieval-augmented LLMs framework--the first of its kind to automatically and strategically extract key evidence from web sources for claim verification.
Employing a multi-round retrieval strategy, our framework ensures the acquisition of sufficient, relevant evidence, thereby enhancing performance.
Comprehensive experiments across three real-world datasets validate the framework's superiority over existing methods. Importantly, our model not only delivers accurate verdicts but also offers human-readable explanations to improve result interpretability.
</code></pre>
</details>
<hr />
<p>41 <a href="https://arxiv.org/abs/2403.09750">[2403.09750] Meta-Cognitive Analysis: Evaluating Declarative and Procedural Knowledge in Datasets and Large Language Models</a></p>
<p>元认知分析:评估数据集和大型语言模型中的陈述性和程序性知识</p>
<details>
<summary>查看更多</summary>
<pre><code>
Date: Thu, 14 Mar 2024 05:34:35 GMT
Authors: Zhuoqun Li, Hongyu Lin, Yaojie Lu, Hao Xiang, Xianpei Han, Le Sun
Abstract
Declarative knowledge and procedural knowledge are two key parts in meta-cognitive theory, and these two hold significant importance in pre-training and inference of LLMs. However, a comprehensive analysis comparing these two types of knowledge is lacking, primarily due to challenges in definition, probing and quantitative assessment. In this paper, we explore from a new perspective by providing ground-truth knowledge for LLMs and evaluating the effective score. Through extensive experiments with widely-used datasets and models, we get conclusions: (1) In most tasks, benefits from declarative knowledge are greater than those from procedural knowledge. (2) Profits of procedural knowledge are larger than declarative knowledge only in reasoning tasks with simple logic. (3) As pre-training progresses and size increases, model ability to utilize both kinds of knowledge significantly improves, but in different speed. We do detailed analysis for the findings and this can provide primary guidance for evaluation and enhancement of large language models.
</code></pre>
</details>
<hr />
<p>43 <a href="https://arxiv.org/abs/2403.09832">[2403.09832] Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks</a></p>
<p>提示注入攻击下大型语言模型机器翻译的扩展行为</p>
<details>
<summary>查看更多</summary>
<pre><code>
Date: Thu, 14 Mar 2024 19:39:10 GMT
Authors: Zhifan Sun and Antonio Valerio Miceli-Barone
Abstract
Large Language Models (LLMs) are increasingly becoming the preferred foundation platforms for many Natural Language Processing tasks such as Machine Translation, owing to their quality often comparable to or better than task-specific models, and the simplicity of specifying the task through natural language instructions or in-context examples. Their generality, however, opens them up to subversion by end users who may embed into their requests instructions that cause the model to behave in unauthorized and possibly unsafe ways. In this work we study these Prompt Injection Attacks (PIAs) on multiple families of LLMs on a Machine Translation task, focusing on the effects of model size on the attack success rates. We introduce a new benchmark data set and we discover that on multiple language pairs and injected prompts written in English, larger models under certain conditions may become more susceptible to successful attacks, an instance of the Inverse Scaling phenomenon (McKenzie et al., 2023). To our knowledge, this is the first work to study non-trivial LLM scaling behaviour in a multi-lingual setting.
</code></pre>
</details>
<hr />
<p>44 <a href="https://arxiv.org/abs/2403.09849">[2403.09849] Self-Consistency Boosts Calibration for Math Reasoning</a></p>
<p>自我一致性促进数学推理校准</p>
<details>
<summary>查看更多</summary>
<pre><code>
Date: Thu, 14 Mar 2024 20:17:10 GMT
Authors: Ante Wang, Linfeng Song, Ye Tian, Baolin Peng, Lifeng Jin, Haitao Mi, Jinsong Su and Dong Yu
Abstract
Calibration, which establishes the correlation between accuracy and model confidence, is important for LLM development. We design three off-the-shelf calibration methods based on self-consistency (Wang et al., 2022) for math reasoning tasks. Evaluation on two popular benchmarks (GSM8K and MathQA) using strong open-source LLMs (Mistral and LLaMA2), our methods better bridge model confidence and accuracy than existing methods based on p(True) (Kadavath et al., 2022) or logit (Kadavath et al., 2022).
</code></pre>
</details>
<hr />
<p>46 <a href="https://arxiv.org/abs/2403.09887">[2403.09887] Sabi\'a-2: A New Generation of Portuguese Large Language Models</a></p>
<p>Sabi\' A -2:新一代葡萄牙大型语言模型</p>
<details>
<summary>查看更多</summary>
<pre><code>
Date: Thu, 14 Mar 2024 21:44:48 GMT
Authors: Thales Sales Almeida, Hugo Abonizio, Rodrigo Nogueira and Ramon Pires
Abstract
We introduce Sabi\'a-2, a family of large language models trained on Portuguese texts. The models are evaluated on a diverse range of exams, including entry-level tests for Brazilian universities, professional certification exams, and graduate-level exams for various disciplines such as accounting, economics, engineering, law and medicine. Our results reveal that our best model so far, Sabi\'a-2 Medium, matches or surpasses GPT-4's performance in 23 out of 64 exams and outperforms GPT-3.5 in 58 out of 64 exams. Notably, specialization has a significant impact on a model's performance without the need to increase its size, allowing us to offer Sabi\'a-2 Medium at a price per token that is 10 times cheaper than GPT-4.
Finally, we identified that math and coding are key abilities that need improvement.
</code></pre>
</details>
<hr />
<p>49 <a href="https://arxiv.org/abs/2403.09919">[2403.09919] Recurrent Drafter for Fast Speculative Decoding in Large Language Models</a></p>
<p>大型语言模型快速推测解码的循环草稿</p>
<details>
<summary>查看更多</summary>
<pre><code>
Date: Thu, 14 Mar 2024 23:40:56 GMT
Authors: Aonan Zhang, Chong Wang, Yi Wang, Xuanyu Zhang, Yunfei Cheng
Abstract
In this paper, we introduce an improved approach of speculative decoding aimed at enhancing the efficiency of serving large language models. Our method capitalizes on the strengths of two established techniques: the classic two-model speculative decoding approach, and the more recent single-model approach, Medusa. Drawing inspiration from Medusa, our approach adopts a single-model strategy for speculative decoding. However, our method distinguishes itself by employing a single, lightweight draft head with a recurrent dependency design, akin in essence to the small, draft model uses in classic speculative decoding, but without the complexities of the full transformer architecture. And because of the recurrent dependency, we can use beam search to swiftly filter out undesired candidates with the draft head. The outcome is a method that combines the simplicity of single-model design and avoids the need to create a data-dependent tree attention structure only for inference in Medusa. We empirically demonstrate the effectiveness of the proposed method on several popular open source language models, along with a comprehensive analysis of the trade-offs involved in adopting this approach.
</code></pre>
</details>
<hr />
<p>51 <a href="https://arxiv.org/abs/2403.09972">[2403.09972] Think Twice Before Assure: Confidence Estimation for Large Language Models through Reflection on Multiple Answers</a></p>
<p>三思而后行:通过对多个答案的反思对大型语言模型进行置信度估计</p>
<details>
<summary>查看更多</summary>
<pre><code>
Date: Fri, 15 Mar 2024 02:38:26 GMT
Authors: Moxin Li, Wenjie Wang, Fuli Feng, Fengbin Zhu, Qifan Wang, Tat-Seng Chua
Abstract
Confidence estimation aiming to evaluate output trustability is crucial for the application of large language models (LLM), especially the black-box ones.
Existing confidence estimation of LLM is typically not calibrated due to the overconfidence of LLM on its generated incorrect answers. Existing approaches addressing the overconfidence issue are hindered by a significant limitation that they merely consider the confidence of one answer generated by LLM. To tackle this limitation, we propose a novel paradigm that thoroughly evaluates the trustability of multiple candidate answers to mitigate the overconfidence on incorrect answers. Building upon this paradigm, we introduce a two-step framework, which firstly instructs LLM to reflect and provide justifications for each answer, and then aggregates the justifications for comprehensive confidence estimation. This framework can be integrated with existing confidence estimation approaches for superior calibration. Experimental results on six datasets of three tasks demonstrate the rationality and effectiveness of the proposed framework.
</code></pre>
</details>
<hr />
<p>53 <a href="https://arxiv.org/abs/2403.10020">[2403.10020] Lost in Overlap: Exploring Watermark Collision in LLMs</a></p>
<p>重叠丢失:探索llm中的水印碰撞</p>
<details>
<summary>查看更多</summary>
<pre><code>
Date: Fri, 15 Mar 2024 05:06:21 GMT
Authors: Yiyang Luo, Ke Lin, Chao Gu
Abstract
The proliferation of large language models (LLMs) in generating content raises concerns about text copyright. Watermarking methods, particularly logit-based approaches, embed imperceptible identifiers into text to address these challenges. However, the widespread use of watermarking across diverse LLMs has led to an inevitable issue known as watermark collision during common tasks like question answering and paraphrasing. This study focuses on dual watermark collisions, where two watermarks are present simultaneously in the same text. The research demonstrates that watermark collision poses a threat to detection performance for detectors of both upstream and downstream watermark algorithms.
</code></pre>
</details>
<hr />
<p>54 <a href="https://arxiv.org/abs/2403.10056">[2403.10056] Don't Half-listen: Capturing Key-part Information in Continual Instruction Tuning</a></p>
<p>不要半听半听:在持续的指令调整中捕捉关键部分的信息</p>
<details>
<summary>查看更多</summary>
<pre><code>
Date: Fri, 15 Mar 2024 06:54:20 GMT
Authors: Yongquan He and Xuancheng Huang and Minghao Tang and Lingxun Meng and Xiang Li and Wei Lin and Wenyuan Zhang and Yifu Gao
Abstract
Instruction tuning for large language models (LLMs) can drive them to produce results consistent with human goals in specific downstream tasks. However, the process of continual instruction tuning (CIT) for LLMs may bring about the catastrophic forgetting (CF) problem, where previously learned abilities are degraded. Recent methods try to alleviate the CF problem by modifying models or replaying data, which may only remember the surface-level pattern of instructions and get confused on held-out tasks. In this paper, we propose a novel continual instruction tuning method based on Key-part Information Gain (KPIG). Our method computes the information gain on masked parts to dynamically replay data and refine the training objective, which enables LLMs to capture task-aware information relevant to the correct response and alleviate overfitting to general descriptions in instructions. In addition, we propose two metrics, P-score and V-score, to measure the generalization and instruction-following abilities of LLMs. Experiments demonstrate our method achieves superior performance on both seen and held-out tasks.
</code></pre>
</details>
<hr />
<p>56 <a href="https://arxiv.org/abs/2403.10081">[2403.10081] DRAGIN: Dynamic Retrieval Augmented Generation based on the Real-time Information Needs of Large Language Models</a></p>
<p>DRAGIN:基于大型语言模型实时信息需求的动态检索增强生成</p>
<details>
<summary>查看更多</summary>
<pre><code>
Date: Fri, 15 Mar 2024 07:45:37 GMT
Authors: Weihang Su, Yichen Tang, Qingyao Ai, Zhijing Wu, Yiqun Liu
Abstract
Dynamic retrieval augmented generation (RAG) paradigm actively decides when and what to retrieve during the text generation process of Large Language Models (LLMs). There are two key elements of this paradigm: identifying the optimal moment to activate the retrieval module (deciding when to retrieve) and crafting the appropriate query once retrieval is triggered (determining what to retrieve). However, current dynamic RAG methods fall short in both aspects.
Firstly, the strategies for deciding when to retrieve often rely on static rules. Moreover, the strategies for deciding what to retrieve typically limit themselves to the LLM's most recent sentence or the last few tokens, while the LLM's real-time information needs may span across the entire context. To overcome these limitations, we introduce a new framework, DRAGIN, i.e., Dynamic Retrieval Augmented Generation based on the real-time Information Needs of LLMs. Our framework is specifically designed to make decisions on when and what to retrieve based on the LLM's real-time information needs during the text generation process. We evaluate DRAGIN along with existing methods comprehensively over 4 knowledge-intensive generation datasets. Experimental results show that DRAGIN achieves superior performance on all tasks, demonstrating the effectiveness of our method. We have open-sourced all the code, data, and models in GitHub: https://github.com/oneal2000/DRAGIN/tree/main
</code></pre>
</details>
<hr />
<p>57 <a href="https://arxiv.org/abs/2403.10088">[2403.10088] Intent-conditioned and Non-toxic Counterspeech Generation using Multi-Task Instruction Tuning with RLAIF</a></p>
<p>基于RLAIF多任务指令调优的意图条件无毒反语音生成</p>
<details>
<summary>查看更多</summary>
<pre><code>
Date: Fri, 15 Mar 2024 08:03:49 GMT
Authors: Amey Hengle, Aswini Kumar, Sahajpreet Singh, Anil Bandhakavi, Md Shad Akhtar, Tanmoy Chakroborty
Abstract
Counterspeech, defined as a response to mitigate online hate speech, is increasingly used as a non-censorial solution. Addressing hate speech effectively involves dispelling the stereotypes, prejudices, and biases often subtly implied in brief, single-sentence statements or abuses. These implicit expressions challenge language models, especially in seq2seq tasks, as model performance typically excels with longer contexts. Our study introduces CoARL, a novel framework enhancing counterspeech generation by modeling the pragmatic implications underlying social biases in hateful statements. CoARL's first two phases involve sequential multi-instruction tuning, teaching the model to understand intents, reactions, and harms of offensive statements, and then learning task-specific low-rank adapter weights for generating intent-conditioned counterspeech. The final phase uses reinforcement learning to fine-tune outputs for effectiveness and non-toxicity. CoARL outperforms existing benchmarks in intent-conditioned counterspeech generation, showing an average improvement of 3 points in intent-conformity and 4 points in argument-quality metrics. Extensive human evaluation supports CoARL's efficacy in generating superior and more context-appropriate responses compared to existing systems, including prominent LLMs like ChatGPT.
</code></pre>
</details>
<hr />
<p>58 <a href="https://arxiv.org/abs/2403.10131">[2403.10131] RAFT: Adapting Language Model to Domain Specific RAG</a></p>
<p>RAFT:面向领域RAG的语言模型</p>
<details>
<summary>查看更多</summary>
<pre><code>
Date: Fri, 15 Mar 2024 09:26:02 GMT
Authors: Tianjun Zhang, Shishir G. Patil, Naman Jain, Sheng Shen, Matei Zaharia, Ion Stoica, Joseph E. Gonzalez
Abstract
Pretraining Large Language Models (LLMs) on large corpora of textual data is now a standard paradigm. When using these LLMs for many downstream applications, it is common to additionally bake in new knowledge (e.g., time-critical news, or private domain knowledge) into the pretrained model either through RAG-based-prompting, or fine-tuning. However, the optimal methodology for the model to gain such new knowledge remains an open question.
In this paper, we present Retrieval Augmented FineTuning (RAFT), a training recipe that improves the model's ability to answer questions in a "open-book" in-domain settings. In RAFT, given a question, and a set of retrieved documents, we train the model to ignore those documents that don't help in answering the question, which we call, distractor documents. RAFT accomplishes this by citing verbatim the right sequence from the relevant document that would help answer the question. This coupled with RAFT's chain-of-thought-style response helps improve the model's ability to reason. In domain-specific RAG, RAFT consistently improves the model's performance across PubMed, HotpotQA, and Gorilla datasets, presenting a post-training recipe to improve pre-trained LLMs to in-domain RAG. RAFT's code and demo are open-sourced at github.com/ShishirPatil/gorilla.
</code></pre>
</details>
<hr />
<p>61 <a href="https://arxiv.org/abs/2403.10205">[2403.10205] Read between the lines -- Functionality Extraction From READMEs</a></p>
<p>字里行间阅读——从自述文件中提取功能</p>
<details>
<summary>查看更多</summary>
<pre><code>
Date: Fri, 15 Mar 2024 11:11:57 GMT
Authors: Prince Kumar, Srikanth Tamilselvam, Dinesh Garg
Abstract
While text summarization is a well-known NLP task, in this paper, we introduce a novel and useful variant of it called functionality extraction from Git README files. Though this task is a text2text generation at an abstract level, it involves its own peculiarities and challenges making existing text2text generation systems not very useful. The motivation behind this task stems from a recent surge in research and development activities around the use of large language models for code-related tasks, such as code refactoring, code summarization, etc. We also release a human-annotated dataset called FuncRead, and develop a battery of models for the task. Our exhaustive experimentation shows that small size fine-tuned models beat any baseline models that can be designed using popular black-box or white-box large language models (LLMs) such as ChatGPT and Bard. Our best fine-tuned 7 Billion CodeLlama model exhibit 70% and 20% gain on the F1 score against ChatGPT and Bard respectively.
</code></pre>
</details>
<hr />
<p>65 <a href="https://arxiv.org/abs/2403.10258">[2403.10258] Is Translation All You Need? A Study on Solving Multilingual Tasks with Large Language Models</a></p>
<p>你只需要翻译吗?用大型语言模型解决多语言任务的研究</p>
<details>
<summary>查看更多</summary>
<pre><code>
Date: Fri, 15 Mar 2024 12:47:39 GMT
Authors: Chaoqun Liu, Wenxuan Zhang, Yiran Zhao, Anh Tuan Luu, Lidong Bing
Abstract
Large language models (LLMs) have demonstrated strong multilingual capabilities; yet, they are mostly English-centric due to the imbalanced training corpora. Existing works leverage this phenomenon to improve their multilingual performances on NLP tasks. In this work, we extend the evaluation from NLP tasks to real user queries. We find that even though translation into English can help improve the performance of multilingual NLP tasks for English-centric LLMs, it may not be optimal for all scenarios. For culture-related tasks that need deep language understanding, prompting in the native language proves to be more promising since it can capture the nuances related to culture and language. Therefore, we advocate for more efforts towards the development of strong multilingual LLMs instead of just English-centric LLMs.
</code></pre>
</details>
<hr />
<p>66 <a href="https://arxiv.org/abs/2403.10275">[2403.10275] A Question on the Explainability of Large Language Models and the Word-Level Univariate First-Order Plausibility Assumption</a></p>
<p>大型语言模型可解释性问题及词级单变量一阶似然假设</p>
<details>
<summary>查看更多</summary>
<pre><code>
Date: Fri, 15 Mar 2024 13:15:23 GMT
Authors: Jeremie Bogaert, Francois-Xavier Standaert
Abstract
The explanations of large language models have recently been shown to be sensitive to the randomness used for their training, creating a need to characterize this sensitivity. In this paper, we propose a characterization that questions the possibility to provide simple and informative explanations for such models. To this end, we give statistical definitions for the explanations' signal, noise and signal-to-noise ratio. We highlight that, in a typical case study where word-level univariate explanations are analyzed with first-order statistical tools, the explanations of simple feature-based models carry more signal and less noise than those of transformer ones. We then discuss the possibility to improve these results with alternative definitions of signal and noise that would capture more complex explanations and analysis methods, while also questioning the tradeoff with their plausibility for readers.
</code></pre>
</details>
<hr />
<p>67 <a href="https://arxiv.org/abs/2403.10281">[2403.10281] Team Trifecta at Factify5WQA: Setting the Standard in Fact Verification with Fine-Tuning</a></p>
<p>Factify5WQA的三重团队:通过微调来设定事实验证的标准</p>
<details>
<summary>查看更多</summary>
<pre><code>
Date: Fri, 15 Mar 2024 13:24:28 GMT
Authors: Shang-Hsuan Chiang, Ming-Chih Lo, Lin-Wei Chao and Wen-Chih Peng
Abstract
In this paper, we present Pre-CoFactv3, a comprehensive framework comprised of Question Answering and Text Classification components for fact verification.
Leveraging In-Context Learning, Fine-tuned Large Language Models (LLMs), and the FakeNet model, we address the challenges of fact verification. Our experiments explore diverse approaches, comparing different Pre-trained LLMs, introducing FakeNet, and implementing various ensemble methods. Notably, our team, Trifecta, secured first place in the AAAI-24 Factify 3.0 Workshop, surpassing the baseline accuracy by 103% and maintaining a 70% lead over the second competitor. This success underscores the efficacy of our approach and its potential contributions to advancing fact verification research.
</code></pre>
</details>
<hr />
<p>69 <a href="https://arxiv.org/abs/2403.10301">[2403.10301] Uni-SMART: Universal Science Multimodal Analysis and Research Transformer</a></p>
<p>Uni-SMART:通用科学多模态分析与研究Transformer</p>
<details>
<summary>查看更多</summary>
<pre><code>
Date: Fri, 15 Mar 2024 13:43:47 GMT
Authors: Hengxing Cai, Xiaochen Cai, Shuwen Yang, Jiankun Wang, Lin Yao, Zhifeng Gao, Junhan Chang, Sihang Li, Mingjun Xu, Changxin Wang, Hongshuai Wang, Yongge Li, Mujie Lin, Yaqi Li, Yuqi Yin, Linfeng Zhang, Guolin Ke
Abstract
In scientific research and its application, scientific literature analysis is crucial as it allows researchers to build on the work of others. However, the fast growth of scientific knowledge has led to a massive increase in scholarly articles, making in-depth literature analysis increasingly challenging and time-consuming. The emergence of Large Language Models (LLMs) has offered a new way to address this challenge. Known for their strong abilities in summarizing texts, LLMs are seen as a potential tool to improve the analysis of scientific literature. However, existing LLMs have their own limits. Scientific literature often includes a wide range of multimodal elements, such as molecular structure, tables, and charts, which are hard for text-focused LLMs to understand and analyze. This issue points to the urgent need for new solutions that can fully understand and analyze multimodal content in scientific literature. To answer this demand, we present Uni-SMART (Universal Science Multimodal Analysis and Research Transformer), an innovative model designed for in-depth understanding of multimodal scientific literature. Through rigorous quantitative evaluation across several domains, Uni-SMART demonstrates superior performance over leading text-focused LLMs. Furthermore, our exploration extends to practical applications, including patent infringement detection and nuanced analysis of charts. These applications not only highlight Uni-SMART's adaptability but also its potential to revolutionize how we interact with scientific literature.
</code></pre>
</details>
<hr />
<p>72 <a href="https://arxiv.org/abs/2403.10351">[2403.10351] TriSum: Learning Summarization Ability from Large Language Models with Structured Rationale</a></p>
<p>TriSum:基于结构化理性的大型语言模型摘要能力学习</p>
<details>
<summary>查看更多</summary>
<pre><code>
Date: Fri, 15 Mar 2024 14:36:38 GMT
Authors: Pengcheng Jiang, Cao Xiao, Zifeng Wang, Parminder Bhatia, Jimeng Sun, Jiawei Han
Abstract
The advent of large language models (LLMs) has significantly advanced natural language processing tasks like text summarization. However, their large size and computational demands, coupled with privacy concerns in data transmission, limit their use in resource-constrained and privacy-centric settings. To overcome this, we introduce TriSum, a framework for distilling LLMs' text summarization abilities into a compact, local model. Initially, LLMs extract a set of aspect-triple rationales and summaries, which are refined using a dual-scoring method for quality. Next, a smaller local model is trained with these tasks, employing a curriculum learning strategy that evolves from simple to complex tasks. Our method enhances local model performance on various benchmarks (CNN/DailyMail, XSum, and ClinicalTrial), outperforming baselines by 4.5%, 8.5%, and 7.4%, respectively. It also improves interpretability by providing insights into the summarization rationale.
</code></pre>
</details>
<hr />
<p>75 <a href="https://arxiv.org/abs/2403.10446">[2403.10446] Enhancing LLM Factual Accuracy with RAG to Counter Hallucinations: A Case Study on Domain-Specific Queries in Private Knowledge-Bases</a></p>
<p>用RAG增强LLM的事实准确性以对抗幻觉:私有知识库中特定领域查询的案例研究</p>
<details>
<summary>查看更多</summary>
<pre><code>
Date: Fri, 15 Mar 2024 16:30:14 GMT
Authors: Jiarui Li and Ye Yuan and Zehua Zhang
Abstract
We proposed an end-to-end system design towards utilizing Retrieval Augmented Generation (RAG) to improve the factual accuracy of Large Language Models (LLMs) for domain-specific and time-sensitive queries related to private knowledge-bases. Our system integrates RAG pipeline with upstream datasets processing and downstream performance evaluation. Addressing the challenge of LLM hallucinations, we finetune models with a curated dataset which originates from CMU's extensive resources and annotated with the teacher model. Our experiments demonstrate the system's effectiveness in generating more accurate answers to domain-specific and time-sensitive inquiries. The results also revealed the limitations of fine-tuning LLMs with small-scale and skewed datasets. This research highlights the potential of RAG systems in augmenting LLMs with external datasets for improved performance in knowledge-intensive tasks. Our code and models are available on Github.
</code></pre>
</details>
<hr />
<p>116 <a href="https://arxiv.org/abs/2403.10444">[2403.10444] Optimal Block-Level Draft Verification for Accelerating Speculative Decoding</a></p>
<p>加速推测解码的最优块级草案验证</p>
<details>
<summary>查看更多</summary>
<pre><code>
Date: Fri, 15 Mar 2024 16:28:22 GMT
Authors: Ziteng Sun and Jae Hun Ro and Ahmad Beirami and Ananda Theertha Suresh
Abstract
Speculative decoding has shown to be an effective method for lossless acceleration of large language models (LLMs) during inference. In each iteration, the algorithm first uses a smaller model to draft a block of tokens.
The tokens are then verified by the large model in parallel and only a subset of tokens will be kept to guarantee that the final output follows the distribution of the large model. In all of the prior speculative decoding works, the draft verification is performed token-by-token independently. In this work, we propose a better draft verification algorithm that provides additional wall-clock speedup without incurring additional computation cost and draft tokens. We first formulate the draft verification step as a block-level optimal transport problem. The block-level formulation allows us to consider a wider range of draft verification algorithms and obtain a higher number of accepted tokens in expectation in one draft block. We propose a verification algorithm that achieves the optimal accepted length for the block-level transport problem. We empirically evaluate our proposed block-level verification algorithm in a wide range of tasks and datasets, and observe consistent improvements in wall-clock speedup when compared to token-level verification algorithm. To the best of our knowledge, our work is the first to establish improvement over speculative decoding through a better draft verification algorithm.
</code></pre>
</details>
<hr />
<p>128 <a href="https://arxiv.org/abs/2403.09717">[2403.09717] Enhancing Depression-Diagnosis-Oriented Chat with Psychological State Tracking</a></p>
<p>利用心理状态追踪增强面向抑郁症诊断的聊天</p>
<details>
<summary>查看更多</summary>
<pre><code>
Date: Tue, 12 Mar 2024 07:17:01 GMT
Authors: Yiyang Gu, Yougen Zhou, Qin Chen, Ningning Zhou, Jie Zhou, Aimin Zhou, Liang He
Abstract
Depression-diagnosis-oriented chat aims to guide patients in self-expression to collect key symptoms for depression detection. Recent work focuses on combining task-oriented dialogue and chitchat to simulate the interview-based depression diagnosis. Whereas, these methods can not well capture the changing information, feelings, or symptoms of the patient during dialogues. Moreover, no explicit framework has been explored to guide the dialogue, which results in some useless communications that affect the experience. In this paper, we propose to integrate Psychological State Tracking (POST) within the large language model (LLM) to explicitly guide depression-diagnosis-oriented chat.
Specifically, the state is adapted from a psychological theoretical model, which consists of four components, namely Stage, Information, Summary and Next.
We fine-tune an LLM model to generate the dynamic psychological state, which is further used to assist response generation at each turn to simulate the psychiatrist. Experimental results on the existing benchmark show that our proposed method boosts the performance of all subtasks in depression-diagnosis-oriented chat.
</code></pre>
</details>
<hr />
<p>130 <a href="https://arxiv.org/abs/2403.09740">[2403.09740] Teaching Machines to Code: Smart Contract Translation with LLMs</a></p>
<p>教学机器编码:基于LLMs的智能合约翻译</p>
<details>
<summary>查看更多</summary>
<pre><code>
Date: Wed, 13 Mar 2024 18:55:20 GMT
Authors: Rabimba Karanjai, Lei Xu, Weidong Shi
Abstract
The advent of large language models (LLMs) has marked a significant milestone in the realm of artificial intelligence, with their capabilities often matching or surpassing human expertise in various domains. Among these achievements, their adeptness in translation tasks stands out, closely mimicking the intricate and preliminary processes undertaken by human translators to ensure the fidelity and quality of the translated content. Despite the advancements in utilizing LLMs for translating programming code across different languages, the domain of smart contract translation, particularly into languages not previously encountered by the LLM, remains largely unexplored. In our research, we present a pioneering approach, SolMover, which harnesses the synergy of two distinct LLMs within a unified framework. This framework is designed to grasp coding principles and apply this understanding to the translation of code into an unfamiliar language. Our study delves into the capacity of LLMs to mimic human learning processes, offering an in-depth evaluation of our methodology for converting smart contracts written in Solidity to Move, a language with limited resources. The framework employs one LLM to decipher coding conventions for the new language, creating a blueprint for the second LLM, which, lacking planning abilities, possesses coding expertise. The empirical evidence from our experiments suggests that SolMover substantially enhances performance compared to gpt-3.5-turbo-1106, and achieves superior results over competitors such as Palm2 and Mixtral-8x7B-Instruct. Additionally, our analysis highlights the efficacy of our bug mitigation strategy in elevating code quality across all models, even outside the SolMover framework.
</code></pre>
</details>
<hr />
<p>131 <a href="https://arxiv.org/abs/2403.09751">[2403.09751] What Was Your Prompt? A Remote Keylogging Attack on AI Assistants</a></p>
<p>你的提示是什么?对AI助手的远程键盘记录攻击</p>
<details>
<summary>查看更多</summary>
<pre><code>
Date: Thu, 14 Mar 2024 09:38:12 GMT
Authors: Roy Weiss, Daniel Ayzenshteyn, Guy Amit, Yisroel Mirsky
Abstract
AI assistants are becoming an integral part of society, used for asking advice or help in personal and confidential issues. In this paper, we unveil a novel side-channel that can be used to read encrypted responses from AI Assistants over the web: the token-length side-channel. We found that many vendors, including OpenAI and Microsoft, have this side-channel.
However, inferring the content of a response from a token-length sequence alone proves challenging. This is because tokens are akin to words, and responses can be several sentences long leading to millions of grammatically correct sentences. In this paper, we show how this can be overcome by (1) utilizing the power of a large language model (LLM) to translate these sequences, (2) providing the LLM with inter-sentence context to narrow the search space and (3) performing a known-plaintext attack by fine-tuning the model on the target model's writing style.
Using these methods, we were able to accurately reconstruct 29\% of an AI assistant's responses and successfully infer the topic from 55\% of them. To demonstrate the threat, we performed the attack on OpenAI's ChatGPT-4 and Microsoft's Copilot on both browser and API traffic.
</code></pre>
</details>
<hr />
<p>135 <a href="https://arxiv.org/abs/2403.09795">[2403.09795] Helpful or Harmful? Exploring the Efficacy of Large Language Models for Online Grooming Prevention</a></p>
<p>有益还是有害?探索大型语言模型对在线梳妆预防的功效</p>
<details>
<summary>查看更多</summary>
<pre><code>
Date: Thu, 14 Mar 2024 18:27:43 GMT
Authors: Ellie Prosser and Matthew Edwards
Abstract
Powerful generative Large Language Models (LLMs) are becoming popular tools amongst the general public as question-answering systems, and are being utilised by vulnerable groups such as children. With children increasingly interacting with these tools, it is imperative for researchers to scrutinise the safety of LLMs, especially for applications that could lead to serious outcomes, such as online child safety queries. In this paper, the efficacy of LLMs for online grooming prevention is explored both for identifying and avoiding grooming through advice generation, and the impact of prompt design on model performance is investigated by varying the provided context and prompt specificity. In results reflecting over 6,000 LLM interactions, we find that no models were clearly appropriate for online grooming prevention, with an observed lack of consistency in behaviours, and potential for harmful answer generation, especially from open-source models. We outline where and how models fall short, providing suggestions for improvement, and identify prompt designs that heavily altered model performance in troubling ways, with findings that can be used to inform best practice usage guides.
</code></pre>
</details>
<hr />
<p>153 <a href="https://arxiv.org/abs/2403.10086">[2403.10086] Large Language Models to Generate System-Level Test Programs Targeting Non-functional Properties</a></p>
<p>基于大型语言模型生成针对非功能属性的系统级测试程序</p>
<details>
<summary>查看更多</summary>
<pre><code>
Date: Fri, 15 Mar 2024 08:01:02 GMT
Authors: Denis Schwachhofer, Peter Domanski, Steffen Becker, Stefan Wagner, Matthias Sauer, Dirk Pfl\"uger, Ilia Polian
Abstract
System-Level Test (SLT) has been a part of the test flow for integrated circuits for over a decade and still gains importance. However, no systematic approaches exist for test program generation, especially targeting non-functional properties of the Device under Test (DUT). Currently, test engineers manually compose test suites from off-the-shelf software, approximating the end-user environment of the DUT. This is a challenging and tedious task that does not guarantee sufficient control over non-functional properties. This paper proposes Large Language Models (LLMs) to generate test programs. We take a first glance at how pre-trained LLMs perform in test program generation to optimize non-functional properties of the DUT. Therefore, we write a prompt to generate C code snippets that maximize the instructions per cycle of a super-scalar, out-of-order architecture in simulation.
Additionally, we apply prompt and hyperparameter optimization to achieve the best possible results without further training.
</code></pre>
</details>
<hr />
<p>155 <a href="https://arxiv.org/abs/2403.10107">[2403.10107] Enhancing Human-Centered Dynamic Scene Understanding via Multiple LLMs Collaborated Reasoning</a></p>
<p>通过多个llm协同推理增强以人为中心的动态场景理解</p>
<details>
<summary>查看更多</summary>
<pre><code>
Date: Fri, 15 Mar 2024 08:51:15 GMT
Authors: Hang Zhang, Wenxiao Zhang, Haoxuan Qu, Jun Liu
Abstract
Human-centered dynamic scene understanding plays a pivotal role in enhancing the capability of robotic and autonomous systems, in which Video-based Human-Object Interaction (V-HOI) detection is a crucial task in semantic scene understanding, aimed at comprehensively understanding HOI relationships within a video to benefit the behavioral decisions of mobile robots and autonomous driving systems. Although previous V-HOI detection models have made significant strides in accurate detection on specific datasets, they still lack the general reasoning ability like human beings to effectively induce HOI relationships. In this study, we propose V-HOI Multi-LLMs Collaborated Reasoning (V-HOI MLCR), a novel framework consisting of a series of plug-and-play modules that could facilitate the performance of current V-HOI detection models by leveraging the strong reasoning ability of different off-the-shelf pre-trained large language models (LLMs). We design a two-stage collaboration system of different LLMs for the V-HOI task. Specifically, in the first stage, we design a Cross-Agents Reasoning scheme to leverage the LLM conduct reasoning from different aspects.
In the second stage, we perform Multi-LLMs Debate to get the final reasoning answer based on the different knowledge in different LLMs. Additionally, we devise an auxiliary training strategy that utilizes CLIP, a large vision-language model to enhance the base V-HOI models' discriminative ability to better cooperate with LLMs. We validate the superiority of our design by demonstrating its effectiveness in improving the prediction accuracy of the base V-HOI model via reasoning from multiple perspectives.
</code></pre>
</details>
<hr />
<p>156 <a href="https://arxiv.org/abs/2403.10135">[2403.10135] The Whole is Better than the Sum: Using Aggregated Demonstrations in In-Context Learning for Sequential Recommendation</a></p>
<p>整体优于总和:在上下文学习中使用聚合演示进行顺序推荐</p>
<details>
<summary>查看更多</summary>
<pre><code>
Date: Fri, 15 Mar 2024 09:28:19 GMT
Authors: Lei Wang, Ee-Peng Lim
Abstract
Large language models (LLMs) have shown excellent performance on various NLP tasks. To use LLMs as strong sequential recommenders, we explore the in-context learning approach to sequential recommendation. We investigate the effects of instruction format, task consistency, demonstration selection, and number of demonstrations. As increasing the number of demonstrations in ICL does not improve accuracy despite using a long prompt, we propose a novel method called LLMSRec-Syn that incorporates multiple demonstration users into one aggregated demonstration. Our experiments on three recommendation datasets show that LLMSRec-Syn outperforms state-of-the-art LLM-based sequential recommendation methods. In some cases, LLMSRec-Syn can perform on par with or even better than supervised learning methods. Our code is publicly available at https://github.com/demoleiwang/LLMSRec_Syn.
</code></pre>
</details>
<hr />
<p>164 <a href="https://arxiv.org/abs/2403.10228">[2403.10228] HawkEye: Training Video-Text LLMs for Grounding Text in Videos</a></p>
<p>HawkEye:训练视频-文本llm，用于视频中的文本基础</p>
<details>
<summary>查看更多</summary>
<pre><code>
Date: Fri, 15 Mar 2024 11:58:18 GMT
Authors: Yueqian Wang, Xiaojun Meng, Jianxin Liang, Yuxuan Wang, Qun Liu, Dongyan Zhao
Abstract
Video-text Large Language Models (video-text LLMs) have shown remarkable performance in answering questions and holding conversations on simple videos.
However, they perform almost the same as random on grounding text queries in long and complicated videos, having little ability to understand and reason about temporal information, which is the most fundamental difference between videos and images. In this paper, we propose HawkEye, one of the first video-text LLMs that can perform temporal video grounding in a fully text-to-text manner. To collect training data that is applicable for temporal video grounding, we construct InternVid-G, a large-scale video-text corpus with segment-level captions and negative spans, with which we introduce two new time-aware training objectives to video-text LLMs. We also propose a coarse-grained method of representing segments in videos, which is more robust and easier for LLMs to learn and follow than other alternatives. Extensive experiments show that HawkEye is better at temporal video grounding and comparable on other video-text tasks with existing video-text LLMs, which verifies its superior video-text multi-modal understanding abilities.
</code></pre>
</details>
<hr />
<p>177 <a href="https://arxiv.org/abs/2403.10482">[2403.10482] Can a GPT4-Powered AI Agent Be a Good Enough Performance Attribution Analyst?</a></p>
<p>一个gpt4驱动的AI智能体能成为一个足够好的性能归因分析师吗?</p>
<details>
<summary>查看更多</summary>
<pre><code>
Date: Fri, 15 Mar 2024 17:12:57 GMT
Authors: Bruno de Melo
Abstract
Performance attribution analysis, defined as the process of explaining the drivers of the excess performance of an investment portfolio against a benchmark, stands as a significant aspect of portfolio management and plays a crucial role in the investment decision-making process, particularly within the fund management industry. Rooted in a solid financial and mathematical framework, the importance and methodologies of this analytical technique are extensively documented across numerous academic research papers and books. The integration of large language models (LLMs) and AI agents marks a groundbreaking development in this field. These agents are designed to automate and enhance the performance attribution analysis by accurately calculating and analyzing portfolio performances against benchmarks. In this study, we introduce the application of an AI Agent for a variety of essential performance attribution tasks, including the analysis of performance drivers and utilizing LLMs as calculation engine for multi-level attribution analysis and question-answer (QA) exercises. Leveraging advanced prompt engineering techniques such as Chain-of-Thought (CoT) and Plan and Solve (PS), and employing a standard agent framework from LangChain, the research achieves promising results: it achieves accuracy rates exceeding 93% in analyzing performance drivers, attains 100% in multi-level attribution calculations, and surpasses 84% accuracy in QA exercises that simulate official examination standards. These findings affirm the impactful role of AI agents, prompt engineering and evaluation in advancing portfolio management processes, highlighting a significant advancement in the practical application and evaluation of AI technologies within the domain.
</code></pre>
</details>
<hr />
<p>181 <a href="https://arxiv.org/abs/2403.10517">[2403.10517] VideoAgent: Long-form Video Understanding with Large Language Model as Agent</a></p>
<p>VideoAgent:以大型语言模型为代理的长篇视频理解</p>
<details>
<summary>查看更多</summary>
<pre><code>
Date: Fri, 15 Mar 2024 17:57:52 GMT
Authors: Xiaohan Wang, Yuhui Zhang, Orr Zohar and Serena Yeung-Levy
Abstract
Long-form video understanding represents a significant challenge within computer vision, demanding a model capable of reasoning over long multi-modal sequences. Motivated by the human cognitive process for long-form video understanding, we emphasize interactive reasoning and planning over the ability to process lengthy visual inputs. We introduce a novel agent-based system, VideoAgent, that employs a large language model as a central agent to iteratively identify and compile crucial information to answer a question, with vision-language foundation models serving as tools to translate and retrieve visual information. Evaluated on the challenging EgoSchema and NExT-QA benchmarks, VideoAgent achieves 54.1% and 71.3% zero-shot accuracy with only 8.4 and 8.2 frames used on average. These results demonstrate superior effectiveness and efficiency of our method over the current state-of-the-art methods, highlighting the potential of agent-based approaches in advancing long-form video understanding.
</code></pre>
</details>
<hr />
<p>183 <a href="https://arxiv.org/abs/2403.09792">[2403.09792] Images are Achilles' Heel of Alignment: Exploiting Visual Vulnerabilities for Jailbreaking Multimodal Large Language Models</a></p>
<p>图像是对齐的阿喀琉斯之踵:利用视觉漏洞为多模态大型语言模型越狱</p>
<details>
<summary>查看更多</summary>
<pre><code>
Date: Thu, 14 Mar 2024 18:24:55 GMT
Authors: Yifan Li, Hangyu Guo, Kun Zhou, Wayne Xin Zhao and Ji-Rong Wen
Abstract
In this paper, we study the harmlessness alignment problem of multimodal large language models~(MLLMs). We conduct a systematic empirical analysis of the harmlessness performance of representative MLLMs and reveal that the image input poses the alignment vulnerability of MLLMs. Inspired by this, we propose a novel jailbreak method named HADES, which hides and amplifies the harmfulness of the malicious intent within the text input, using meticulously crafted images. Experimental results show that HADES can effectively jailbreak existing MLLMs, which achieves an average Attack Success Rate~(ASR) of 90.26% for LLaVA-1.5 and 71.60% for Gemini Pro Vision. Our code and data will be publicly released.
</code></pre>
</details>
<hr />
<p>202 <a href="https://arxiv.org/abs/2403.10153">[2403.10153] Improving Medical Multi-modal Contrastive Learning with Expert Annotations</a></p>
<p>利用专家注释改进医学多模态对比学习</p>
<details>
<summary>查看更多</summary>
<pre><code>
Date: Fri, 15 Mar 2024 09:54:04 GMT
Authors: Yogesh Kumar, Pekka Marttinen
Abstract
We introduce eCLIP, an enhanced version of the CLIP model that integrates expert annotations in the form of radiologist eye-gaze heatmaps. It tackles key challenges in contrastive multi-modal medical imaging analysis, notably data scarcity and the "modality gap" -- a significant disparity between image and text embeddings that diminishes the quality of representations and hampers cross-modal interoperability. eCLIP integrates a heatmap processor and leverages mixup augmentation to efficiently utilize the scarce expert annotations, thus boosting the model's learning effectiveness. eCLIP is designed to be generally applicable to any variant of CLIP without requiring any modifications of the core architecture. Through detailed evaluations across several tasks, including zero-shot inference, linear probing, cross-modal retrieval, and Retrieval Augmented Generation (RAG) of radiology reports using a frozen Large Language Model, eCLIP showcases consistent improvements in embedding quality. The outcomes reveal enhanced alignment and uniformity, affirming eCLIP's capability to harness high-quality annotations for enriched multi-modal analysis in the medical imaging domain.
</code></pre>
</details>
<hr />
<p>210 <a href="https://arxiv.org/abs/2403.10408">[2403.10408] SocialGenPod: Privacy-Friendly Generative AI Social Web Applications with Decentralised Personal Data Stores</a></p>
<p>SocialGenPod:具有分散个人数据存储的隐私友好的生成式AI社交网络应用程序</p>
<details>
<summary>查看更多</summary>
<pre><code>
Date: Fri, 15 Mar 2024 15:43:02 GMT
Authors: Vidminas Vizgirda (1), Rui Zhao (2), and Naman Goel (2) ((1) University of Edinburgh, (2) University of Oxford)
Abstract
We present SocialGenPod, a decentralised and privacy-friendly way of deploying generative AI Web applications. Unlike centralised Web and data architectures that keep user data tied to application and service providers, we show how one can use Solid -- a decentralised Web specification -- to decouple user data from generative AI applications. We demonstrate SocialGenPod using a prototype that allows users to converse with different Large Language Models, optionally leveraging Retrieval Augmented Generation to generate answers grounded in private documents stored in any Solid Pod that the user is allowed to access, directly or indirectly. SocialGenPod makes use of Solid access control mechanisms to give users full control of determining who has access to data stored in their Pods. SocialGenPod keeps all user data (chat history, app configuration, personal documents, etc) securely in the user's personal Pod; separate from specific model or application providers. Besides better privacy controls, this approach also enables portability across different services and applications. Finally, we discuss challenges, posed by the large compute requirements of state-of-the-art models, that future research in this area should address. Our prototype is open-source and available at: https://github.com/Vidminas/socialgenpod/.
</code></pre>
</details>
<hr />
<p>219 <a href="https://arxiv.org/abs/2311.10112">[2311.10112] zrLLM: Zero-Shot Relational Learning on Temporal Knowledge Graphs with Large Language Models</a></p>
<p>zrLLM:基于大规模语言模型的时序知识图谱零样本关系学习</p>
<details>
<summary>查看更多</summary>
<pre><code>
Date: replaced with revised version Fri, 15 Mar 2024 15:38:07 GMT
Authors: Zifeng Ding, Heling Cai, Jingpei Wu, Yunpu Ma, Ruotong Liao, Bo Xiong, Volker Tresp
Abstract
Categories
</code></pre>
</details>
<hr />
<p>221 <a href="https://arxiv.org/abs/2402.05359">[2402.05359] Prompting Large Language Models with Divide-and-Conquer Program for Discerning Problem Solving</a></p>
<p>基于分而治之的大型语言模型识别问题求解</p>
<details>
<summary>查看更多</summary>
<pre><code>
Date: replaced with revised version Thu, 14 Mar 2024 21:12:42 GMT
Authors: Yizhou Zhang, Lun Du, Defu Cao, Qiang Fu, Yan Liu
Abstract
Categories
</code></pre>
</details>
<hr />
<p>234 <a href="https://arxiv.org/abs/2304.11657">[2304.11657] Enhancing Chain-of-Thoughts Prompting with Iterative Bootstrapping in Large Language Models</a></p>
<p>用迭代自助法增强大型语言模型中的思维链提示</p>
<details>
<summary>查看更多</summary>
<pre><code>
Date: replaced with revised version Fri, 15 Mar 2024 10:28:13 GMT
Authors: Jiashuo Sun and Yi Luo and Yeyun Gong and Chen Lin and Yelong Shen and Jian Guo and Nan Duan
Abstract
Categories
</code></pre>
</details>
<hr />
<p>236 <a href="https://arxiv.org/abs/2308.07317">[2308.07317] Platypus: Quick, Cheap, and Powerful Refinement of LLMs</a></p>
<p>Platypus:快速、廉价、强大的llm改进</p>
<details>
<summary>查看更多</summary>
<pre><code>
Date: replaced with revised version Thu, 14 Mar 2024 20:56:23 GMT
Authors: Ariel N. Lee, Cole J. Hunter, Nataniel Ruiz
Abstract
Categories
</code></pre>
</details>
<hr />
<p>244 <a href="https://arxiv.org/abs/2311.07445">[2311.07445] Think Before You Speak: Cultivating Communication Skills of Large Language Models via Inner Monologue</a></p>
<p>说话前先思考:通过内心独白培养大型语言模型的沟通技巧</p>
<details>
<summary>查看更多</summary>
<pre><code>
Date: replaced with revised version Fri, 15 Mar 2024 08:30:30 GMT
Authors: Junkai Zhou, Liang Pang, Huawei Shen, Xueqi Cheng
Abstract
Categories
</code></pre>
</details>
<hr />
<p>246 <a href="https://arxiv.org/abs/2311.09022">[2311.09022] Exploring the Potential of Large Language Models in Computational Argumentation</a></p>
<p>探索大型语言模型在计算辩论中的潜力</p>
<details>
<summary>查看更多</summary>
<pre><code>
Date: replaced with revised version Fri, 15 Mar 2024 10:00:04 GMT
Authors: Guizhen Chen, Liying Cheng, Luu Anh Tuan, Lidong Bing
Abstract
Categories
</code></pre>
</details>
<hr />
<p>248 <a href="https://arxiv.org/abs/2401.01989">[2401.01989] Revisiting Zero-Shot Abstractive Summarization in the Era of Large Language Models from the Perspective of Position Bias</a></p>
<p>从立场偏差的角度重新审视大规模语言模型时代的零样本抽象摘要</p>
<details>
<summary>查看更多</summary>
<pre><code>
Date: replaced with revised version Thu, 14 Mar 2024 23:20:33 GMT
Authors: Anshuman Chhabra, Hadi Askari, Prasant Mohapatra
Abstract
Categories
</code></pre>
</details>
<hr />
<p>252 <a href="https://arxiv.org/abs/2402.15302">[2402.15302] How (un)ethical are instruction-centric responses of LLMs? Unveiling the vulnerabilities of safety guardrails to harmful queries</a></p>
<p>llm以指令为中心的反应(un)如何符合伦理?揭示安全护栏对有害查询的脆弱性</p>
<details>
<summary>查看更多</summary>
<pre><code>
Date: replaced with revised version Fri, 15 Mar 2024 17:57:58 GMT
Authors: Somnath Banerjee, Sayan Layek, Rima Hazra, Animesh Mukherjee
Abstract
Categories
</code></pre>
</details>
<hr />
<p>253 <a href="https://arxiv.org/abs/2402.16363">[2402.16363] LLM Inference Unveiled: Survey and Roofline Model Insights</a></p>
<p>LLM推断公布:调查和Roofline模型见解</p>
<details>
<summary>查看更多</summary>
<pre><code>
Date: replaced with revised version Fri, 15 Mar 2024 01:58:58 GMT
Authors: Zhihang Yuan, Yuzhang Shang, Yang Zhou, Zhen Dong, Zhe Zhou, Chenhao Xue, Bingzhe Wu, Zhikai Li, Qingyi Gu, Yong Jae Lee, Yan Yan, Beidi Chen, Guangyu Sun, Kurt Keutzer
Abstract
Categories
</code></pre>
</details>
<hr />
<p>254 <a href="https://arxiv.org/abs/2403.01976">[2403.01976] SciAssess: Benchmarking LLM Proficiency in Scientific Literature Analysis</a></p>
<p>sciassessment: LLM科学文献分析能力的基准测试</p>
<details>
<summary>查看更多</summary>
<pre><code>
Date: replaced with revised version Fri, 15 Mar 2024 13:27:31 GMT
Authors: Hengxing Cai, Xiaochen Cai, Junhan Chang, Sihang Li, Lin Yao, Changxin Wang, Zhifeng Gao, Hongshuai Wang, Yongge Li, Mujie Lin, Shuwen Yang, Jiankun Wang, Yuqi Yin, Yaqi Li, Linfeng Zhang, Guolin Ke
Abstract
Categories
</code></pre>
</details>
<hr />
<p>257 <a href="https://arxiv.org/abs/2403.07378">[2403.07378] SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model Compression</a></p>
<p>SVD-LLM:基于截断感知奇异值分解的大型语言模型压缩</p>
<details>
<summary>查看更多</summary>
<pre><code>
Date: replaced with revised version Fri, 15 Mar 2024 02:59:10 GMT
Authors: Xin Wang, Yu Zheng, Zhongwei Wan, Mi Zhang
Abstract
Categories
</code></pre>
</details>
<hr />
<p>260 <a href="https://arxiv.org/abs/2403.09539">[2403.09539] Logits of API-Protected LLMs Leak Proprietary Information</a></p>
<p>api保护的llm的logit泄漏了专有信息</p>
<details>
<summary>查看更多</summary>
<pre><code>
Date: replaced with revised version Fri, 15 Mar 2024 02:07:30 GMT
Authors: Matthew Finlayson, Xiang Ren, Swabha Swayamdipta
Abstract
Categories
</code></pre>
</details>
<hr />
<p>314 <a href="https://arxiv.org/abs/2312.14125">[2312.14125] VideoPoet: A Large Language Model for Zero-Shot Video Generation</a></p>
<p>VideoPoet:用于零样本视频生成的大型语言模型</p>
<details>
<summary>查看更多</summary>
<pre><code>
Date: replaced with revised version Thu, 14 Mar 2024 18:08:11 GMT
Authors: Dan Kondratyuk and Lijun Yu and Xiuye Gu and Jos\'e Lezama and Jonathan Huang and Rachel Hornung and Hartwig Adam and Hassan Akbari and Yair Alon and Vighnesh Birodkar and Yong Cheng and Ming-Chang Chiu and Josh Dillon and Irfan Essa and Agrim Gupta and Meera Hahn and Anja Hauth and David Hendon and Alonso Martinez and David Minnen and David Ross and Grant Schindler and Mikhail Sirotenko and Kihyuk Sohn and Krishna Somandepalli and Huisheng Wang and Jimmy Yan and Ming-Hsuan Yang and Xuan Yang and Bryan Seybold and Lu Jiang
Abstract
Categories
</code></pre>
</details>
<hr />
<p>321 <a href="https://arxiv.org/abs/2403.03230">[2403.03230] Large language models surpass human experts in predicting neuroscience results</a></p>
<p>大型语言模型在预测神经科学结果方面超过了人类专家</p>
<details>
<summary>查看更多</summary>
<pre><code>
Date: replaced with revised version Thu, 14 Mar 2024 23:32:15 GMT
Authors: Xiaoliang Luo, Akilles Rechardt, Guangzhi Sun, Kevin K. Nejad, Felipe Y\'a\~nez, Bati Yilmaz, Kangjoo Lee, Alexandra O. Cohen, Valentina Borghesani, Anton Pashkov, Daniele Marinazzo, Jonathan Nicholas, Alessandro Salatiello, Ilia Sucholutsky, Pasquale Minervini, Sepehr Razavi, Roberta Rocca, Elkhan Yusifov, Tereza Okalova, Nianlong Gu, Martin Ferianc, Mikail Khona, Kaustubh R. Patil, Pui-Shee Lee, Rui Mata, Nicholas E. Myers, Jennifer K Bizley, Sebastian Musslick, Isil Poyraz Bilgin, Guiomar Niso, Justin M. Ales, Michael Gaebler, N Apurva Ratan Murty, Leyla Loued-Khenissi, Anna Behler, Chloe M. Hall, Jessica Dafflon, Sherry Dongqi Bao, Bradley C. Love
Abstract
Categories
</code></pre>
</details>
<hr />
<p>328 <a href="https://arxiv.org/abs/2310.02992">[2310.02992] Kosmos-G: Generating Images in Context with Multimodal Large Language Models</a></p>
<p>Kosmos-G:用多模态大型语言模型在上下文中生成图像</p>
<details>
<summary>查看更多</summary>
<pre><code>
Date: replaced with revised version Fri, 15 Mar 2024 04:38:21 GMT
Authors: Xichen Pan, Li Dong, Shaohan Huang, Zhiliang Peng, Wenhu Chen, Furu Wei
Abstract
Categories
</code></pre>
</details>
<hr />
<p>329 <a href="https://arxiv.org/abs/2402.13607">[2402.13607] CODIS: Benchmarking Context-Dependent Visual Comprehension for Multimodal Large Language Models</a></p>
<p>CODIS:多模态大型语言模型上下文相关视觉理解基准测试</p>
<details>
<summary>查看更多</summary>
<pre><code>
Date: replaced with revised version Fri, 15 Mar 2024 11:19:30 GMT
Authors: Fuwen Luo, Chi Chen, Zihao Wan, Zhaolu Kang, Qidong Yan, Yingjie Li, Xiaolong Wang, Siyu Wang, Ziyue Wang, Xiaoyue Mi, Peng Li, Ning Ma, Maosong Sun, Yang Liu
Abstract
Categories
</code></pre>
</details>
<hr />